{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    '''\n",
    "    Function that returns the tanh of x\n",
    "\n",
    "    Input: \n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    tanh(x)\n",
    "    '''\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    '''\n",
    "    Function that returns the derivative of tanh(x)\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    tanh_derivative(x)\n",
    "    '''\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def ReLU(x):\n",
    "    '''\n",
    "    Function that returns the ReLU of x\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    ReLU(x)\n",
    "    '''\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    '''\n",
    "    Function that returns the derivative of ReLU(x)\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    ReLU_derivative(x)\n",
    "    '''\n",
    "    return np.where(x>0,1,0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    Function that returns the sigmoid of x\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    sigmoid(x)\n",
    "    '''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    '''\n",
    "    Function that returns the derivative of sigmoid(x)\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    sigmoid_derivative(x)\n",
    "    '''\n",
    "    Sig_x = sigmoid(x)\n",
    "    return Sig_x*(1-Sig_x)\n",
    "\n",
    "def LeakyReLU(x):\n",
    "    '''\n",
    "    Function that returns the LeakyReLU of x\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    LeakyReLU(x)\n",
    "    '''\n",
    "    return np.where(x>0,x,0.01*x)\n",
    "\n",
    "def LeakyReLU_derivative(x):\n",
    "    '''\n",
    "    Function that returns the derivative of LeakyReLU(x)\n",
    "\n",
    "    Input:\n",
    "    x: A scalar or numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    LeakyReLU_derivative(x)\n",
    "    '''\n",
    "    return np.where(x>0,1,0.01)\n",
    "\n",
    "def Convert_Output_To_Probability_Distribution(x):\n",
    "    '''\n",
    "    Function that returns the probability distribution of the output\n",
    "\n",
    "    Input:\n",
    "    x: A numpy array of any size.\n",
    "\n",
    "    Output:\n",
    "    exps: Probability Distribution of the output\n",
    "    '''\n",
    "    exps = np.exp(np.float128(x))\n",
    "    return exps/np.sum(exps,dtype=np.float128)\n",
    "\n",
    "def SoftMax(ytrue,pred):\n",
    "    '''\n",
    "    Function that returns the loss of the output\n",
    "\n",
    "    Input:\n",
    "    ytrue: A numpy array of actual labels\n",
    "    pred: A numpy array of predictions\n",
    "\n",
    "    Output:\n",
    "    output: Loss of the output\n",
    "    '''\n",
    "    Actual = pred[ytrue[0][0]]\n",
    "    Numerator = np.exp(Actual)\n",
    "    Denominator = np.sum(np.exp(np.float128(pred)),dtype=np.float128)\n",
    "    output = (-1 * np.log(Numerator / Denominator))\n",
    "    return output\n",
    "\n",
    "def SoftMax_derivative(ytrue,pred):\n",
    "    '''\n",
    "    Function that returns the derivative of the loss of the output\n",
    "\n",
    "    Input:\n",
    "    ytrue: A numpy array of actual labels\n",
    "    pred: A numpy array of predictions\n",
    "\n",
    "    Output:\n",
    "    Gradients: Derivative of the loss of the output\n",
    "    '''\n",
    "    pred = Convert_Output_To_Probability_Distribution(pred)\n",
    "    Gradients = []\n",
    "    for idx,output in enumerate(pred):\n",
    "        if idx == ytrue[0][0]:\n",
    "            Gradients.append(output - 1)\n",
    "        else:\n",
    "            Gradients.append(output)\n",
    "    return [Gradients]\n",
    "\n",
    "def SVM_Loss(ytrue,pred):\n",
    "    '''\n",
    "    Function that returns the loss of the output\n",
    "\n",
    "    Input:\n",
    "    ytrue: A numpy array of actual labels\n",
    "    pred: A numpy array of predictions\n",
    "\n",
    "    Output:\n",
    "    Res: Loss of the output\n",
    "    '''\n",
    "    Actual = pred[ytrue[0][0]]\n",
    "    Res = 0.0\n",
    "    for idx,output in enumerate(pred):\n",
    "        if idx != ytrue[0][0]:\n",
    "            RightSide = np.float32(output - Actual + 1)\n",
    "            Res += np.max((0,RightSide))\n",
    "    return Res\n",
    "\n",
    "def SVM_Loss_derivative(ytrue,pred):\n",
    "    '''\n",
    "    Function that returns the derivative of the loss of the output\n",
    "\n",
    "    Input:\n",
    "    ytrue: A numpy array of actual labels\n",
    "    pred: A numpy array of predictions\n",
    "\n",
    "    Output:\n",
    "    Gradients: Derivative of the loss of the output\n",
    "    '''\n",
    "    Gradients = []\n",
    "    Actual = pred[ytrue[0][0]]\n",
    "    for idx,output in enumerate(pred):\n",
    "        if idx == ytrue[0][0]:\n",
    "            Gradients.append(0)\n",
    "        else:\n",
    "            if output - Actual + 1 > 0:\n",
    "                Gradients.append(1)\n",
    "            else:\n",
    "                Gradients.append(0)\n",
    "    return [Gradients]\n",
    "\n",
    "class Fully_Connected_Layer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Constructor for the Fully Connected Layer that takes input size and output size as input of the layer\n",
    "\n",
    "        Input:\n",
    "        input_size: Size of the input\n",
    "        output_size: Size of the output\n",
    "\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        self.weights = np.random.uniform(-1,1,(input_size, output_size)) #Initialize weights with random values in range -1 to 1\n",
    "        self.bias = np.zeros((1, output_size)) #Initialize bias with 0\n",
    "                             \n",
    "    def Forward_Propagation(self, input_data):\n",
    "        '''\n",
    "        Function that returns the output of the layer\n",
    "\n",
    "        Input:\n",
    "        input_data: A numpy array of any size.\n",
    "\n",
    "        Output:\n",
    "        self.output: Output of the layer\n",
    "        '''\n",
    "        self.input = input_data #Store the input for backpropagation\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias #Calculate the output of the respective layer\n",
    "        return self.output #Return the output of the layer\n",
    "\n",
    "    def Backward_Propagation(self, output_error, lr):\n",
    "        '''\n",
    "        Function that performs backpropagation and returns the error of the layer\n",
    "\n",
    "        Input:\n",
    "        output_error: A numpy array of any size.\n",
    "\n",
    "        Output:\n",
    "        input_error: Backpropogation result of the layer\n",
    "        '''\n",
    "        input_error = np.dot(output_error, self.weights.T) #Dot product with weights because equation in forward propagation is WX + b i.e. derivative of WX w.r.t X is W\n",
    "        weights_error = np.dot(self.input.T, output_error) #Dot product with input because equation in forward propagation is WX + b i.e. derivative of WX w.r.t W is X\n",
    "        dBias = output_error #Since biases have local derivative 1 so, it will be same as the output error as it is added to the output of the layer in forward propagation\n",
    "\n",
    "        #Updating the weights and biases of the layer\n",
    "        self.weights -= (lr * weights_error)\n",
    "        self.bias -= (lr * np.array(dBias))\n",
    "        return input_error\n",
    "    \n",
    "class Activation_Layer():\n",
    "    '''\n",
    "    Class that defines the activation layer\n",
    "    '''\n",
    "    def __init__(self, activation, activation_derivative):\n",
    "        '''\n",
    "        Constructor for the activation layer that takes activation function and its derivative as input\n",
    "\n",
    "        Input:\n",
    "        activation: Activation function\n",
    "        activation_derivative: Derivative of the activation function\n",
    "\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "    def Forward_Propagation(self, input_data):\n",
    "        '''\n",
    "        Function that returns the output of the layer\n",
    "\n",
    "        Input:\n",
    "        input_data: A numpy array of any size.\n",
    "\n",
    "        Output:\n",
    "        self.output: Output of the layer after applying activation function\n",
    "        '''\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def Backward_Propagation(self, output_error, learning_rate): #Learning rate is not meant to be used here but it is here to avoid repeating statements for different layers\n",
    "        '''\n",
    "        Function that performs backpropagation and returns the error of the layer\n",
    "\n",
    "        Input:\n",
    "        output_error: A numpy array of any size.\n",
    "\n",
    "        Output:\n",
    "        input_error: Backpropogation result of the layer\n",
    "        '''\n",
    "        return self.activation_derivative(self.input) * output_error\n",
    "\n",
    "class Neural_Network:\n",
    "    '''\n",
    "    Class that defines the neural Neural_Network architecture\n",
    "    '''\n",
    "    def __init__(self,LossFunction, LossFunction_Derivative):\n",
    "        '''\n",
    "        Constructor for the neural Neural_Network that takes no input\n",
    "\n",
    "        Input:\n",
    "        LossFunction: Loss function to be used\n",
    "        LossFunction_Derivative: Derivative of the loss function to be used\n",
    "\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        self.layers = [] #List to store the layers of the Neural_Network\n",
    "        self.loss = LossFunction #Loss function\n",
    "        self.loss_prime = LossFunction_Derivative #Derivative of the loss function\n",
    "\n",
    "    def insert_layer(self, layer):\n",
    "        '''\n",
    "        Function that adds the layer to the Neural_Network\n",
    "\n",
    "        Input:\n",
    "        layer: Layer to be added to the Neural_Network\n",
    "\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        self.layers.append(layer) #Add the layer to the list of layers\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        '''\n",
    "        Function that returns the output of the Neural_Network after predictions\n",
    "\n",
    "        Input:\n",
    "        input_data: A numpy array of any size.\n",
    "\n",
    "        Output:\n",
    "        result: Output of the Neural_Network after predictions\n",
    "        '''\n",
    "        samples = len(input_data) #Get the number of test examples\n",
    "        result = [] #List to store the output of the Neural_Network\n",
    "\n",
    "        #Iterating for each test example\n",
    "        for i in range(samples):\n",
    "            output = input_data[i][0]\n",
    "            for layer in self.layers: #Pass the test example through each layer of the Neural_Network\n",
    "                output = layer.Forward_Propagation(output)\n",
    "            result.append(output) #Append the output of the output layer of the Neural_Network to the list\n",
    "        return result #Return the list of outputs\n",
    "\n",
    "    def fit(self, x_train, y_train, n_Epochs, learning_rate=0.01):\n",
    "        '''\n",
    "        Function to train the Neural_Network\n",
    "\n",
    "        Input:\n",
    "        x_train: Training data\n",
    "        y_train: Training labels\n",
    "        epochs: Number of epochs to train the Neural_Network\n",
    "        learning_rate: Learning rate of the Neural_Network\n",
    "\n",
    "        Output:\n",
    "        None\n",
    "        '''\n",
    "        NumExamples = len(x_train)\n",
    "        \n",
    "        #Training loop\n",
    "        for i in range(n_Epochs):\n",
    "            RunningLoss = 0.0 #Running Error for each epoch\n",
    "            for j in range(NumExamples):\n",
    "                #Forward Propagation\n",
    "                Result = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    Result = layer.Forward_Propagation(Result) #Pass the training example through each layer of the Neural_Network\n",
    "\n",
    "                #Compute loss (for display purpose only)\n",
    "                TempLoss = self.loss(y_train[j], Result[0])\n",
    "                RunningLoss += TempLoss\n",
    "\n",
    "                if self.loss == SVM_Loss and TempLoss == 0.0:\n",
    "                    '''\n",
    "                    If the loss is SVM_Loss and the loss is 0.0 then there is no need to perform backpropagation to update the weights and biases\n",
    "                    '''\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    #Backward propagation\n",
    "                    error = self.loss_prime(y_train[j], Result[0])\n",
    "                    for layer in reversed(self.layers):\n",
    "                        error = layer.Backward_Propagation(error, learning_rate) #Pass the error through each layer of the Neural_Network in reverse order\n",
    "                    \n",
    "            #Calculate average error on all samples\n",
    "            RunningLoss /= NumExamples #Average error\n",
    "            print('Epoch:', i+1, 'Loss:', RunningLoss) #Printing the epoch number and the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1       2       3       4       5    6    7    8    9  ...   55  \\\n",
       "0  0.0  0.0  0.3125  0.8125  0.5625  0.0625  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1  0.0  0.0  0.0000  0.7500  0.8125  0.3125  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2  0.0  0.0  0.0000  0.2500  0.9375  0.7500  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "3  0.0  0.0  0.4375  0.9375  0.8125  0.0625  0.0  0.0  0.0  0.5  ...  0.0   \n",
       "4  0.0  0.0  0.0000  0.0625  0.6875  0.0000  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "    56   57      58      59      60      61      62   63  target  \n",
       "0  0.0  0.0  0.3750  0.8125  0.6250  0.0000  0.0000  0.0       0  \n",
       "1  0.0  0.0  0.0000  0.6875  1.0000  0.6250  0.0000  0.0       1  \n",
       "2  0.0  0.0  0.0000  0.1875  0.6875  1.0000  0.5625  0.0       2  \n",
       "3  0.0  0.0  0.4375  0.8125  0.8125  0.5625  0.0000  0.0       3  \n",
       "4  0.0  0.0  0.0000  0.1250  1.0000  0.2500  0.0000  0.0       4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the required libraries\n",
    "X,Y = load_digits(return_X_y=True)\n",
    "\n",
    "#Normalizing the data\n",
    "X = (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "\n",
    "DF = pd.DataFrame(X)\n",
    "DF['target'] = Y\n",
    "DF.to_csv('data.csv')\n",
    "DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 1, 64) (360, 1, 64) (1437, 1, 1) (360, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing data\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, stratify=Y,random_state=42,shuffle=True)\n",
    "\n",
    "#Reshaping the data\n",
    "x_train = x_train.reshape((np.shape(x_train)[0],1,np.shape(x_train)[1]))\n",
    "x_test = x_test.reshape((np.shape(x_test)[0],1,np.shape(x_test)[1]))\n",
    "y_train = y_train.reshape((np.shape(y_train)[0],1,1))\n",
    "y_test = y_test.reshape((np.shape(y_test)[0],1,1))\n",
    "print(np.shape(x_train),np.shape(x_test),np.shape(y_train),np.shape(y_test)) #Printing the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Accuracy(Actual,Pred):\n",
    "    '''\n",
    "    Function to calculate the accuracy of the model\n",
    "    '''\n",
    "    count = 0\n",
    "    Total = 0\n",
    "    for idx,actual in enumerate(Actual):\n",
    "        if actual[0][0] == np.argmax(Pred[idx][0]):\n",
    "            count += 1\n",
    "        Total += 1\n",
    "    return (count/Total)*100 #Returning the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss being used: SoftMax\n",
      "Epoch: 1 Loss: 1.5771268453136747495\n",
      "Epoch: 2 Loss: 0.71707562545964573686\n",
      "Epoch: 3 Loss: 0.46468617766372714674\n",
      "Epoch: 4 Loss: 0.34864079316294555535\n",
      "Epoch: 5 Loss: 0.2800157578257109242\n",
      "Epoch: 6 Loss: 0.23406156913045814598\n",
      "Epoch: 7 Loss: 0.20089067172392036463\n",
      "Epoch: 8 Loss: 0.17570075827173978088\n",
      "Epoch: 9 Loss: 0.15582061618959241499\n",
      "Epoch: 10 Loss: 0.13964699991594215164\n",
      "Epoch: 11 Loss: 0.12616210259579738324\n",
      "Epoch: 12 Loss: 0.11469128894484502669\n",
      "Epoch: 13 Loss: 0.104776943103330936826\n",
      "Epoch: 14 Loss: 0.09610440157645325042\n",
      "Epoch: 15 Loss: 0.0884531116789761604\n",
      "Accuracy: 95.55555555555556\n",
      "Loss being used: SVM_Loss\n",
      "Epoch: 1 Loss: 9.524972621172605\n",
      "Epoch: 2 Loss: 9.199299868638214\n",
      "Epoch: 3 Loss: 9.221789978790278\n",
      "Epoch: 4 Loss: 9.205218124052012\n",
      "Epoch: 5 Loss: 9.186366232469398\n",
      "Epoch: 6 Loss: 9.206279598338343\n",
      "Epoch: 7 Loss: 9.245864870902942\n",
      "Epoch: 8 Loss: 9.245948348685298\n",
      "Epoch: 9 Loss: 9.245996098137132\n",
      "Epoch: 10 Loss: 9.246041758218132\n",
      "Epoch: 11 Loss: 9.246099610084334\n",
      "Epoch: 12 Loss: 9.246207641334925\n",
      "Epoch: 13 Loss: 9.237733098140488\n",
      "Epoch: 14 Loss: 9.204271414101822\n",
      "Epoch: 15 Loss: 9.201422754426602\n",
      "Accuracy: 10.0\n",
      "The better performing loss function is: SoftMax\n"
     ]
    }
   ],
   "source": [
    "Loss_Method_Accuracy = {}\n",
    "for idxLoss,(Loss,LossDer) in enumerate(zip([SoftMax,SVM_Loss],[SoftMax_derivative,SVM_Loss_derivative])):\n",
    "    print('Loss being used:',Loss.__name__)\n",
    "\n",
    "    MNIST_NN = Neural_Network(Loss,LossDer)\n",
    "    MNIST_NN.insert_layer(Fully_Connected_Layer(64, 128))                \n",
    "    MNIST_NN.insert_layer(Activation_Layer(sigmoid, sigmoid_derivative))\n",
    "    MNIST_NN.insert_layer(Fully_Connected_Layer(128, 128))\n",
    "    MNIST_NN.insert_layer(Activation_Layer(sigmoid, sigmoid_derivative))\n",
    "    MNIST_NN.insert_layer(Fully_Connected_Layer(128, 128))\n",
    "    MNIST_NN.insert_layer(Activation_Layer(sigmoid, sigmoid_derivative))\n",
    "    MNIST_NN.insert_layer(Fully_Connected_Layer(128, 128))\n",
    "    MNIST_NN.insert_layer(Activation_Layer(sigmoid, sigmoid_derivative))\n",
    "    MNIST_NN.insert_layer(Fully_Connected_Layer(128, 10))\n",
    "\n",
    "    MNIST_NN.fit(x_train, y_train, n_Epochs=15, learning_rate=0.005)\n",
    "\n",
    "    preds = MNIST_NN.predict(x_test)\n",
    "\n",
    "    Accuracy = Get_Accuracy(y_test,preds)\n",
    "    Loss_Method_Accuracy[Loss.__name__] = Accuracy\n",
    "    print('Accuracy:',Accuracy)\n",
    "\n",
    "print('The better performing loss function is:',max(Loss_Method_Accuracy, key=Loss_Method_Accuracy.get))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
